# Quasi-Newton-Optimization-Project

Implementation of:
 - BFGS
 - L‑BFGS (two‑loop recursion, memory m)
 - DFP
 - SR1
 - Broyden rank‑1
and baselines:
 - Gradient Descent
 - Newton's Method

Experiments:
 1. Convex quadratic minimization (varying condition number & dimension)
 2. Rosenbrock function minimization
 3. Logistic regression (binary, Iris dataset)

Dependencies: numpy, matplotlib, scikit‑learn (for Iris dataset only)**
